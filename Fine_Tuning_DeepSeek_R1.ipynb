{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOmN0iqo2OoeIujnG30bVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayesha9811/Streamlit-Dataset-Creator-Sinhala/blob/main/Fine_Tuning_DeepSeek_R1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up"
      ],
      "metadata": {
        "id": "GwV6DZWPHYvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "HzdQrxV1HVIs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Define configurations for loading the model\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Automatically choose the best data type (float16, bfloat16, etc.)\n",
        "load_in_4bit = True  # Enable 4-bit quantization to reduce memory usage\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "y5_kbWTlRzSN",
        "outputId": "a7a66c90-41b6-4dd6-c3b0-3df149b41c67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b9b0777f493f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define configurations for loading the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (controls low-rank approximation quality)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Layers to apply LoRA\n",
        "    lora_alpha=16, # Scaling factor for LoRA weights\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")"
      ],
      "metadata": {
        "id": "i0YRLpdxHtBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset  # Load datasets from Hugging Face Hub\n",
        "\n",
        "# Load a dataset\n",
        "dataset = load_dataset(\"AyeshaKalpani98/Sinhala-QnA-Generate\", split=\"train\")"
      ],
      "metadata": {
        "id": "Auz-fuGVimjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# Convert dataset format from ShareGPT format to Hugging Face's standardized (\"role\", \"content\") structure\n",
        "dataset = [{\"conversations\": entry[\"messages\"]} for entry in dataset if \"messages\" in entry]\n",
        "\n"
      ],
      "metadata": {
        "id": "alV9VyzrIj6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What's the capital of France?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"The capital of France is Paris.\"}\n",
        "\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}"
      ],
      "metadata": {
        "id": "K3kuZckiQJhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "khwaLyBuE7fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the actual list from the nested structure\n",
        "#dataset = Dataset.from_dict({\"conversations\": [entry[\"conversations\"] for entry in dataset]})\n"
      ],
      "metadata": {
        "id": "j3go4JU4ImCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])  # Should print a list of conversation dictionaries\n"
      ],
      "metadata": {
        "id": "hjY_kJpJIqaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(type(dataset))\n",
        "#print(dataset.column_names)\n",
        "#print(dataset[0])\n"
      ],
      "metadata": {
        "id": "7KZHSQFKI8Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for convo in examples:\n",
        "        formatted_text = tokenizer.apply_chat_template(convo[\"conversations\"], tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(formatted_text)\n",
        "    return {\"text\": texts}\n",
        "\n"
      ],
      "metadata": {
        "id": "rF2Oer1rJN-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Ensure `dataset` is a list of dictionaries before converting\n",
        "if isinstance(dataset, list):\n",
        "    dataset = Dataset.from_list(dataset)  # ✅ Convert list of dicts to Hugging Face Dataset\n",
        "\n",
        "# Apply the Llama-3.1 chat template to the tokenizer\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,  # Ensure `tokenizer` is defined before this step\n",
        "    chat_template=\"llama-3.1\",  # The chat template format\n",
        ")\n",
        "\n",
        "# Function to format the conversation data into tokenized text\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for convos in examples[\"conversations\"]:  # ✅ Iterate over a batch\n",
        "        text = tokenizer.apply_chat_template(convos, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# ✅ Apply `.map()` on the Hugging Face Dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "GLHclGVXkbRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print an item in its original conversation format\n",
        "print(dataset[0][\"conversations\"])\n",
        "\n",
        "# Print the same item in its formatted text format\n",
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "6YZ5dCFbfpwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "from trl import SFTTrainer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Llama-8B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Llama-8B\")\n",
        "\n",
        "# Apply the Llama-3.1 chat template to the tokenizer\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "\n",
        "# Set the max_seq_length (if not defined earlier)\n",
        "max_seq_length = 512  # Adjust the sequence length as per your needs\n",
        "\n",
        "# Define training configurations\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # Number of examples per GPU batch\n",
        "        gradient_accumulation_steps=4,  # Accumulate gradients over 4 batches before updating model\n",
        "        warmup_steps=5,  # Number of warmup steps for learning rate schedule\n",
        "        max_steps=60,  # Limit training steps to 60 (for quick testing)\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,  # Log training metrics after every step\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",  # Linear decay of learning rate\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",  # Directory to save model checkpoints\n",
        "        report_to=\"none\",  # Use this for WandB etc\n",
        "    ),\n",
        ")\n",
        "\n",
        "# You can now start training by calling trainer.train() or other necessary training steps\n"
      ],
      "metadata": {
        "id": "iJ5JJyEyL2Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",  # Mark user input\n",
        "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",  # Mark assistant response\n",
        ")\n",
        "# Start training the model\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "p4ayBiv7MF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(\n",
        "   tokenizer,\n",
        "   chat_template = \"llama-3.1\",\n",
        ")\n",
        "# Set the PAD token to be the same as the EOS token to avoid tokenization issues\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "   {\"role\": \"user\", \"content\": \"I am sad because I failed my Maths test today\"}]\n",
        "# Tokenize the user input with the chat template\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "   messages,\n",
        "   tokenize=True,\n",
        "   add_generation_prompt=True,\n",
        "   return_tensors=\"pt\",\n",
        "   padding=True,  # Add padding to match sequence lengths\n",
        ").to(\"cuda\")\n",
        "\n",
        "attention_mask = inputs != tokenizer.pad_token_id\n",
        "\n",
        "outputs = model.generate(\n",
        "   input_ids=inputs,\n",
        "   attention_mask=attention_mask,\n",
        "   max_new_tokens=64,\n",
        "   use_cache=True,  # Use cache for faster token generation\n",
        "   temperature=0.6,  # Controls randomness in responses\n",
        "   min_p=0.1,  # Set minimum probability threshold for token selection\n",
        ")\n",
        "\n",
        "# Decode the generated tokens into human-readable text\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "o1BwNghFMPlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7e5FawuMPdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}